{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import defaultdict\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "from pprint import pprint\n",
    "from collections import defaultdict\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "right_left_mapping = {\n",
    "    \"schnell\": \"fast\",\n",
    "    \"langsam\": \"slow\",\n",
    "    \"effizient\": \"efficient\",\n",
    "    \"ineffizient\": \"inefficient\",\n",
    "    \"vorhersagbar\": \"predictable\",\n",
    "    \"unberechenbar\": \"unpredictable\",\n",
    "    \"unterstützend\": \"supportive\",\n",
    "    \"behindernd\":\"obstructive\",\n",
    "    \"hilfreich\": \"helpful\",\n",
    "    \"nicht hilfreich\": \"not helpful\",\n",
    "    \"lohnend\": \"rewarding\",\n",
    "    \"nicht lohnend\":\"not rewarding\",\n",
    "    \"mühelos\": \"easy\",\n",
    "    \"mühevoll\": \"difficult\",\n",
    "    \"logisch\": \"logical\",\n",
    "    \"unlogisch\": \"illogical\",\n",
    "    \"einleuchtend\": \"plausible\",\n",
    "    \"nicht einleuchtend\": \"not plausible\",\n",
    "    \"passend\": \"suitable\",\n",
    "    \"unpassend\": \"inappropriate\",\n",
    "    \"intelligent\": \"intelligent\",\n",
    "    \"unintelligent\": \"unintelligent\",\n",
    "    \"gut aufbereitet\": \"well prepared\",\n",
    "    \"schlecht aufbereitet\": \"poorly prepared\",\n",
    "    \"genau\": \"ambiguous\",\n",
    "    \"ungenau\": \"unambiguous\",\n",
    "\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_json_file(path: str) -> dict:\n",
    "    with open(path, \"r\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "\n",
    "def results_per_user() -> dict[str, list[dict]]:\n",
    "    # Search all files in the evaluation directory\n",
    "    results_path = \"dev/evaluation/results/\"\n",
    "    results_files = [join(results_path, f) for f in listdir(results_path)]\n",
    "    results_files = [f for f in results_files if isfile(f)]\n",
    "\n",
    "    # Map each participant to their AI and traditional questionnaire\n",
    "    results_per_user: dict[str, list[dict]] = defaultdict(list)\n",
    "    for path in results_files:\n",
    "        user = path.split(\"_\")[0]\n",
    "        user = user.removeprefix(results_path)\n",
    "\n",
    "        results_per_user[user].append(parse_json_file(path))\n",
    "\n",
    "    return results_per_user\n",
    "\n",
    "results_per_user = results_per_user()\n",
    "results_per_user"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def category_results(\n",
    "    results_per_user: dict[str, list[dict]],\n",
    ") -> dict[str, list[(str, int)]]:\n",
    "    # Map a category to the positive (right) value and its difference\n",
    "    category_results: dict[str, list[(str, int)]] = defaultdict(list)\n",
    "\n",
    "    for user, results in results_per_user.items():\n",
    "        ai = [r for r in results if r[\"mode\"] == \"AI\"][0]\n",
    "        traditional = [r for r in results if r[\"mode\"] == \"traditional\"][0]\n",
    "\n",
    "        for ai_section, trad_section in zip(\n",
    "            ai[\"questionnaire\"], traditional[\"questionnaire\"]\n",
    "        ):\n",
    "            for ai_c, trad_c in zip(\n",
    "                ai_section[\"categories\"], trad_section[\"categories\"]\n",
    "            ):\n",
    "                # small number = bad for AI\n",
    "                # large number = good for AI\n",
    "                category_results[ai_section[\"name\"]].append(\n",
    "                    (ai_c[\"right\"], ai_c[\"reply\"] - trad_c[\"reply\"])\n",
    "                )\n",
    "\n",
    "    return category_results\n",
    "\n",
    "category_results = category_results(results_per_user)\n",
    "category_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def dataframes(\n",
    "    results_per_user: dict[str, list[dict]],\n",
    ") -> dict[str, list[(str, int)]]:\n",
    "    ai_rows: list[dict[str, any]] = []\n",
    "    traditional_rows: list[dict[str, any]] = []\n",
    "    improvement_rows: list[dict[str, any]] = []\n",
    "    # Map a category to the positive (right) value and its difference\n",
    "    category_results: dict[str, list[(str, int)]] = defaultdict(list)\n",
    "\n",
    "    for user, results in results_per_user.items():\n",
    "        ai = [r for r in results if r[\"mode\"] == \"AI\"][0]\n",
    "        traditional = [r for r in results if r[\"mode\"] == \"traditional\"][0]\n",
    "\n",
    "        ai_replies = {\"user\": user}\n",
    "        traditional_replies = {\"user\": user}\n",
    "        improvement_replies = {\"user\": user}\n",
    "\n",
    "        for ai_section, trad_section in zip(\n",
    "            ai[\"questionnaire\"], traditional[\"questionnaire\"]\n",
    "        ):\n",
    "            for ai_c, trad_c in zip(\n",
    "                ai_section[\"categories\"], trad_section[\"categories\"]\n",
    "            ):\n",
    "                right = right_left_mapping[ai_c[\"right\"]]\n",
    "                left = right_left_mapping[ai_c[\"left\"]]\n",
    "                ai_replies[f\"{right}/\\n{left}\"] = ai_c[\"reply\"]\n",
    "                traditional_replies[f\"{right}/\\n{left}\"] = trad_c[\"reply\"]\n",
    "                improvement_replies[f\"{right}/\\n{left}\"] = ai_c[\"reply\"] - trad_c[\"reply\"]\n",
    "\n",
    "        ai_rows.append(ai_replies)\n",
    "        traditional_rows.append(traditional_replies)\n",
    "        improvement_rows.append(improvement_replies)\n",
    "\n",
    "    pprint([len(r.values()) for r in ai_rows])\n",
    "    pprint([len(r.values()) for r in traditional_rows])\n",
    "    pprint([len(r.values()) for r in improvement_rows])\n",
    "\n",
    "    traditional_rows = pd.DataFrame.from_records(traditional_rows)\n",
    "    ai_rows = pd.DataFrame.from_records(ai_rows)\n",
    "    improvement_rows = pd.DataFrame.from_records(improvement_rows)\n",
    "\n",
    "    return traditional_rows, ai_rows, improvement_rows\n",
    "\n",
    "\n",
    "trad, ai, improvement = dataframes(results_per_user)\n",
    "trad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_table(df, path:str, title:str, x_label:str,y_label:str):\n",
    "    melted_df = df.melt(\n",
    "        var_name=x_label, value_name=y_label\n",
    "    )\n",
    "\n",
    "    # Set up the plot\n",
    "    plt.figure(figsize=(len(df.columns)-1, 5))\n",
    "    medianprops = dict(linestyle='-', linewidth=1.5, color='blue')\n",
    "    sns.boxplot(\n",
    "        data=melted_df,\n",
    "        x=x_label,\n",
    "        y=y_label,\n",
    "        color=\"white\",  # Color scheme\n",
    "        medianprops=medianprops,\n",
    "        fill=True,\n",
    "        width=0.5,  # Adjust box width\n",
    "        linewidth=1.5,  # Thicker box borders\n",
    "        linecolor=\"black\"\n",
    "    )\n",
    "\n",
    "    # Optional: Add stripplot for actual data points\n",
    "    # sns.stripplot(\n",
    "    #     data=melted_df,\n",
    "    #     x='Category',\n",
    "    #     y='Improvement',\n",
    "    #     color='black',\n",
    "    #     alpha=0.5,         # Transparency\n",
    "    #     jitter=True        # Spread points to avoid overlap\n",
    "    # )\n",
    "\n",
    "    # Customize plot\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel(x_label, fontsize=12)\n",
    "    plt.ylabel(y_label + \" Score\", fontsize=12)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.xticks(rotation=45)  # Rotate x-labels if needed\n",
    "    plt.yticks(range(-4,5))\n",
    "    plt.tight_layout()\n",
    "    plt.axhline(y=0, lw=1, color=\"k\")\n",
    "\n",
    "    plt.savefig(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_table(improvement.iloc[:, 1 : 8], \"thesis/figures/first_trial_results.svg\", \"Improvement Scores by Category\",\"Category\", \"Improvement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_table(improvement.iloc[:, 8 : ], \"thesis/figures/second_trial_results.svg\", \"Improvement Scores by Category\",\"Category\", \"Improvement\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_table(ai.iloc[:, 1 : ], \"thesis/figures/ai_results.svg\", \"Results for the AI approach\",\"Category\", \"Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_table(trad.iloc[:, 1 : ], \"thesis/figures/traditional_results.svg\", \"Results for the traditional approach\",\"Category\", \"Score\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_comparison(\n",
    "    ai_df, traditional_df, path: str, title: str, x_label: str, y_label: str\n",
    "):\n",
    "    # Melt both dataframes to long format\n",
    "    melted_ai_df = ai_df.melt(\n",
    "        var_name=x_label, value_name=y_label\n",
    "    )\n",
    "    melted_ai_df[\"Table\"] = \"AI\"\n",
    "\n",
    "    melted_traditional_df = traditional_df.melt(\n",
    "        var_name=x_label, value_name=y_label\n",
    "    )\n",
    "    melted_traditional_df[\"Table\"] = \"Traditional\"\n",
    "\n",
    "    # Combine the melted dataframes\n",
    "    combined_df = pd.concat([melted_ai_df, melted_traditional_df])\n",
    "    medianprops = dict(linestyle=\"-\", linewidth=1.5, color=\"black\")\n",
    "    # Set up the plot\n",
    "    plt.figure(figsize=(len(ai_df.columns)+2, 5))\n",
    "    sns.boxplot(\n",
    "        data=combined_df,\n",
    "        x=x_label,\n",
    "        medianprops=medianprops,\n",
    "        y=y_label,\n",
    "        width=.6,\n",
    "        hue=\"Table\",  # Differentiate by table\n",
    "        palette=\"Set1\",  # Use a distinct color palette\n",
    "        linewidth=1.5,  # Thicker box borders\n",
    "    )\n",
    "\n",
    "    # Customize plot appearance\n",
    "    plt.title(title, fontsize=14)\n",
    "    plt.xlabel(x_label, fontsize=12)\n",
    "    plt.ylabel(y_label, fontsize=12)\n",
    "    plt.grid(axis=\"y\", linestyle=\"--\", alpha=0.7)\n",
    "    plt.legend(bbox_to_anchor=(.4, 0.2))\n",
    "    plt.xticks(rotation=45)  # Rotate x-labels for better readability\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # Save and display the plot\n",
    "    plt.savefig(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison(\n",
    "    ai_df=ai.iloc[:, 1 : 8],\n",
    "    traditional_df=trad.iloc[:, 1 : 8],\n",
    "    path=\"thesis/figures/first_comparison.svg\",\n",
    "    title=\"Comparison of AI and Traditional Approach\",\n",
    "    x_label=\"Category\",\n",
    "    y_label=\"Score\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_comparison(\n",
    "    ai_df=ai.iloc[:, 8 : ],\n",
    "    traditional_df=trad.iloc[:, 8 : ],\n",
    "    path=\"thesis/figures/second_comparison.svg\",\n",
    "    title=\"Comparison of AI and Traditional Approach\",\n",
    "    x_label=\"Category\",\n",
    "    y_label=\"Score\",\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
